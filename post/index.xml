<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 清晨的阳光</title>
    <link>https://chacefoo.github.io/post/</link>
    <description>Recent content in Posts on 清晨的阳光</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Mar 2021 18:36:22 +0800</lastBuildDate><atom:link href="https://chacefoo.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>【机器学习笔记】提升方法AdaBoost算法</title>
      <link>https://chacefoo.github.io/post/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95adaboost%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 29 Mar 2021 18:36:22 +0800</pubDate>
      
      <guid>https://chacefoo.github.io/post/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95adaboost%E7%AE%97%E6%B3%95/</guid>
      <description>提升方法 一、什么是提升(boosting)方法 ​ 提升方法是一种常用的统计学习方法。在分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性整合，提高分类的性能。
二、 提升方法的基本思路  提升方法的思想是，“三个臭皮匠顶个诸葛亮”，多个弱分类器能够组合成一个强分类器。
 kearns和Valiant提出强可学习和弱可学习的概念。
 在概率近似正确（probably approximately correct，PAC）学习框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习，而且正确率高，则称这个概念是强可学习的。如果正确率仅仅比随机猜想的略好，称这个概念是弱可学习的。
 Schapire证明，强可学习和弱可学习是等价的。
 实践中常常发现弱可学习算法，将弱可学习算法提升为强可学习算法成为研究的重点。
  三、AdaBoost算法 ​ AdaBoost采取加权多数表决的方法。加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用；减小分类误差大的弱分类器的权值，使其在表决中起较小的作用。
​ 现在叙述AdaBoost算法。假设给定一个二分类的训练数据集 \( T=\left \{ (x_1,y_1),(x_2,y_2),\cdots ,(x_N,y_N)\right \} \) 其中，每个样本点由实例和标记组成。示例 \(x_i \in X \subseteq R^n\) ，标记 \(y_i \in \ Y =\left \{ -1,+1\right \}\) ，\(X\) 是实例空间，\(Y\) 是标记集合。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器。并将这些弱分类器线性组合成一个强分类器。
输入：  训练数据集 \(T=\left \{ (x_1,y_1),(x_2,y_2),\cdots ,(x_N,y_N)\right \}\) ，其中 \(x_i \in X \subseteq R^n\) ，\(y_i \in \ Y =\left \{ -1,+1\right \}\)</description>
    </item>
    
    <item>
      <title>噪声、缺失、异常数据的预处理方法</title>
      <link>https://chacefoo.github.io/post/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%99%AA%E5%A3%B0%E7%BC%BA%E5%A4%B1%E5%BC%82%E5%B8%B8%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/</link>
      <pubDate>Fri, 15 Jan 2021 21:14:03 +0800</pubDate>
      
      <guid>https://chacefoo.github.io/post/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%99%AA%E5%A3%B0%E7%BC%BA%E5%A4%B1%E5%BC%82%E5%B8%B8%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/</guid>
      <description>一、噪声的处理 ​	噪声(noise)是被测量的变量的随机误差或方差。我们可以使用基本的数据统计描述技术(例如，盒图或者散点图)和数据可视化方法来识别可能代表噪声的离群点。
1. 分箱(bining) 分箱方法通过考察数据的“近邻”(即周围的值)来光滑有序的数据值。这些有序的值被分布到一些“捅”或箱中。由于分箱方法考察近邻的值，因此它进行局部的光滑。
如上图所示，数据首先排序并被划分到大小为3的等频的箱中。对于用箱均值光滑，箱中每一个值都被替换为箱中的均值。类似的，可以使用用箱中位数光滑或者用箱边界光滑等等。
2. 回归(regression) 可以用一个函数拟合数据来光滑数据。这种技术称之为回归。线性回归涉及找出拟合两个属性(或变量)的“最佳”直线，使得一个属性可以用来预测另一个。多元线性回归是线性回归的扩充，其中涉及的属性多余两个，并且数据拟合到一个多维曲面。
3. 离群点分析(outlier analysis) ​	可以通过如聚类来检测离群点。聚类将类似的值组织成群或“簇”。直观地，落在簇集合之外的值被视为离群点。
二、缺失的处理 1. 用平均值、中值、分位数、众数、随机值等替代。  如果预计该变量对于学习模型效果影响不大，可以对unknown值赋众数，这里认为变量都对学习模型有较大影响，效果一般，因为等于人为增加了噪声，不建议采取此法。 数值型的话，均值和近邻或许是更好的方法。做成哑变量更适合分类、顺序型变量。
2. 用其他变量做预测模型来算出缺失变量。  效果比方法1略好。有一个根本缺陷，如果其他变量和缺失变量无关，则预测的结果无意义。如果预测结果相当准确，则又说明这个变量是没必要加入建模的。一般情况下，介于两者之间。 可以使用数据完整的行作为训练集，以此来预测缺失值。又由于sklearn的模型只能处理数值变量，需要先将分类变量数值化，然后进行预测。 一般使用KNN, Matrix completion等方法预测。
3. 把变量映射到高维空间。  比如性别，有男、女、缺失三种情况，则映射成3个变量：是否男、是否女、是否缺失。连续型变量也可以这样处理。比如Google、百度的CTR预估模型，预处理时会把所有变量都这样处理，达到几亿维。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值、不用考虑线性不可分之类的问题。缺点是计算量大大提升。 而且只有在样本量非常大的时候效果才好，否则会因为过于稀疏，效果很差。 连续变量这么map岂不会产生超多纬数的data？这种也叫one hot. 把取值变成离散特征。
4. 用特殊值标记 引入虚拟变量(dummy variable)来表征是否有缺失，是否有补全。
5. 忽略该行数据。  有一些模型，如随机森林，自身能够处理数据缺失的情况，在这种情况下不需要对缺失数据做任何的处理，这种做法的缺点是在模型的选择上有局限。
6. 删除。 最简单最直接的方法，很多时候也是最有效的方法，这种做法的缺点是可能会导致信息丢失。对于unknown值数量较少的变量可以选择删除。删除有缺失数据的样本，删除有过多缺失数据的特征。
三、异常的处理 1. 删除 ​	这种方法简单易行，但缺点也很明显，首先我们经常会遇到的情况是样本数很少，这种删除会造成样本的不足，其次，直接删除的样本数很多，也可能会改变变量的原有分布，从而造成统计模型不够稳定。
2. 暂且保留，待结合整体模型综合分析 通常我们观测到的异常值，有时在对于整个模型而言，其异常性质并没有观测到的明显，因此最好综合分析一下，像回归分析，我们经常利用残差分布信息来判断模型优劣，残差有没有超出经验范围（+3标准差），呈现什么分布等，另外对于整个模型而言，会有一些指标像Mahalanobis、Cook&amp;rsquo;s、协方差比率等可以提供某条观测或整体的拟合信息，这些指标也会提示分析人员的异常值信息。如果对于整个模型而言，并不是很明显时，建议保留。
3. 使用均值或其他统计量取代 ​	这不失为一种折中的方法，大部分的参数方法是针对均值来建模的，用均值取代，实际上克服了丢失样本的缺陷，但却丢失了样本“特色”，可以说是不大不小的错误。当然如果是时序数据，用于取代的统计量，可供选择的范围就会多一些，可以针对序列选择合适的统计量取代异常值，也较少存在上述问题。
4. 将其视为缺失值，利用统计模型填补 该方法的好处是可以利用现有变量的信息，对异常值（缺失值）填补。不过这里最好要视该异常值（缺失值）的特点而定，例如需视是完全随机缺失、随机缺失还是非随机缺失的不同情况而定。
5. 不做过多处理，根据其性质特点，使用稳健模型加以修饰 如果按参数性质分的话，可以将稳健方法分为参数、非参和半参3种情况，这大致与通常的关于参数的假设、优点一样。
6. 使用抽样技术或模拟技术，接受更合理的标准误等信息 ​	抽样样本所计算出的均值的标准误，一般来说会更合理，这可以有效应对异常值的影响，但前提是原始样本量不能太少（小于10），小样本的结果不够稳定。另外模拟技术可以利用先验分布特征和样本信息来构建事后预测的概率分布，进行事后模拟，这种技术现在发展的很好，在异常值的应对中，表现良好。</description>
    </item>
    
  </channel>
</rss>
